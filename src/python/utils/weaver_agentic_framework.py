#!/usr/bin/env python3
"""
W&B Weave Agentic Framework for Intelligent Training Monitoring
Advanced AI agents for autonomous training optimization and monitoring
"""

import time
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, asdict
from collections import deque
import numpy as np
import torch

try:
    import weave
    import wandb
    WEAVE_AVAILABLE = True
except ImportError:
    WEAVE_AVAILABLE = False

@dataclass
class TrainingState:
    """Comprehensive training state for agent analysis"""
    current_step: int = 0
    current_epoch: int = 0
    loss: float = 0.0
    learning_rate: float = 0.0
    gradient_norm: float = 0.0
    gpu_memory_percent: float = 0.0
    gpu_temperature: float = 0.0
    cpu_percent: float = 0.0
    training_speed: float = 0.0  # steps per second
    model_convergence: str = "unknown"
    stability_score: float = 0.0
    efficiency_score: float = 0.0

@dataclass
class AgentAction:
    """Actions that agents can recommend or execute"""
    action_type: str
    description: str
    priority: str  # high, medium, low
    auto_executable: bool = False
    parameters: Dict[str, Any] = None
    estimated_impact: str = "unknown"
    
    def __post_init__(self):
        if self.parameters is None:
            self.parameters = {}

@dataclass
class AgentAlert:
    """Alerts generated by monitoring agents"""
    alert_type: str
    severity: str  # critical, warning, info
    message: str
    timestamp: datetime
    source_agent: str
    recommended_actions: List[AgentAction]
    auto_resolved: bool = False

class TrainingIntelligenceAgent:
    """Advanced AI agent for intelligent training monitoring and optimization"""
    
    def __init__(self, name: str, agent_type: str = "monitoring"):
        self.name = name
        self.agent_type = agent_type
        self.history = deque(maxlen=1000)
        self.alerts_generated = []
        self.actions_taken = []
        self.learning_patterns = {}
        
        if WEAVE_AVAILABLE:
            self.weave_enabled = True
        else:
            self.weave_enabled = False
            print(f"âš ï¸  Weave not available for agent {name}")
    
    @weave.op() if WEAVE_AVAILABLE else lambda f: f
    def analyze_training_state(self, state: TrainingState) -> Dict[str, Any]:
        """Analyze current training state and provide intelligence"""
        analysis = {
            "agent_name": self.name,
            "timestamp": datetime.now().isoformat(),
            "state_analysis": {},
            "recommendations": [],
            "alerts": [],
            "actions": [],
            "confidence": 0.0
        }
        
        # Store state in history
        self.history.append(asdict(state))
        
        # Perform intelligent analysis
        analysis.update(self._analyze_loss_patterns(state))
        analysis.update(self._analyze_resource_utilization(state))
        analysis.update(self._analyze_training_stability(state))
        analysis.update(self._analyze_convergence_patterns(state))
        
        return analysis
    
    def _analyze_loss_patterns(self, state: TrainingState) -> Dict[str, Any]:
        """Analyze loss patterns and trends"""
        if len(self.history) < 10:
            return {"loss_analysis": "insufficient_data"}
        
        recent_losses = [h.get("loss", 0) for h in list(self.history)[-20:]]
        recommendations = []
        alerts = []
        
        # Trend analysis
        if len(recent_losses) >= 10:
            recent_avg = np.mean(recent_losses[-5:])
            older_avg = np.mean(recent_losses[-10:-5])
            
            if recent_avg > older_avg * 1.1:
                alerts.append(AgentAlert(
                    alert_type="loss_increasing",
                    severity="warning",
                    message=f"Loss trending upward: {recent_avg:.4f} vs {older_avg:.4f}",
                    timestamp=datetime.now(),
                    source_agent=self.name,
                    recommended_actions=[
                        AgentAction(
                            action_type="reduce_learning_rate",
                            description="Reduce learning rate by 50%",
                            priority="medium",
                            auto_executable=True,
                            parameters={"factor": 0.5}
                        ),
                        AgentAction(
                            action_type="add_regularization",
                            description="Add L2 regularization to prevent overfitting",
                            priority="low",
                            auto_executable=False,
                            parameters={"weight_decay": 0.01}
                        )
                    ]
                ))
        
        # Loss stability
        if len(recent_losses) >= 5:
            loss_std = np.std(recent_losses[-5:])
            loss_mean = np.mean(recent_losses[-5:])
            cv = loss_std / loss_mean if loss_mean > 0 else float('inf')
            
            if cv > 0.1:  # High coefficient of variation
                alerts.append(AgentAlert(
                    alert_type="loss_instability",
                    severity="warning",
                    message=f"High loss variance detected (CV: {cv:.3f})",
                    timestamp=datetime.now(),
                    source_agent=self.name,
                    recommended_actions=[
                        AgentAction(
                            action_type="gradient_clipping",
                            description="Apply gradient clipping to stabilize training",
                            priority="high",
                            auto_executable=True,
                            parameters={"max_norm": 1.0}
                        )
                    ]
                ))
        
        return {
            "loss_analysis": {
                "trend": "increasing" if recent_avg > older_avg else "stable",
                "stability": "unstable" if cv > 0.1 else "stable",
                "recent_losses": recent_losses[-5:],
                "recommendations": recommendations
            },
            "alerts": alerts
        }
    
    def _analyze_resource_utilization(self, state: TrainingState) -> Dict[str, Any]:
        """Analyze GPU and system resource utilization"""
        recommendations = []
        alerts = []
        
        # GPU memory analysis
        if state.gpu_memory_percent > 0.95:
            alerts.append(AgentAlert(
                alert_type="gpu_memory_critical",
                severity="critical",
                message=f"GPU memory critical: {state.gpu_memory_percent:.1%}",
                timestamp=datetime.now(),
                source_agent=self.name,
                recommended_actions=[
                    AgentAction(
                        action_type="reduce_batch_size",
                        description="Automatically reduce batch size",
                        priority="high",
                        auto_executable=True,
                        parameters={"reduction_factor": 0.75}
                    ),
                    AgentAction(
                        action_type="enable_gradient_checkpointing",
                        description="Enable gradient checkpointing to save memory",
                        priority="medium",
                        auto_executable=True
                    )
                ]
            ))
        elif state.gpu_memory_percent < 0.3:
            recommendations.append(
                "GPU memory underutilized. Consider increasing batch size for better efficiency."
            )
        
        # Temperature monitoring
        if state.gpu_temperature > 85:
            alerts.append(AgentAlert(
                alert_type="gpu_temperature_high",
                severity="warning",
                message=f"GPU temperature high: {state.gpu_temperature}Â°C",
                timestamp=datetime.now(),
                source_agent=self.name,
                recommended_actions=[
                    AgentAction(
                        action_type="reduce_workload",
                        description="Temporarily reduce training intensity",
                        priority="medium",
                        auto_executable=True,
                        parameters={"intensity_factor": 0.8}
                    )
                ]
            ))
        
        # CPU utilization
        if state.cpu_percent > 95:
            alerts.append(AgentAlert(
                alert_type="cpu_bottleneck",
                severity="warning",
                message=f"CPU bottleneck detected: {state.cpu_percent:.1f}%",
                timestamp=datetime.now(),
                source_agent=self.name,
                recommended_actions=[
                    AgentAction(
                        action_type="reduce_data_workers",
                        description="Reduce number of data loading workers",
                        priority="medium",
                        auto_executable=True,
                        parameters={"max_workers": 2}
                    )
                ]
            ))
        
        return {
            "resource_analysis": {
                "gpu_memory_status": "critical" if state.gpu_memory_percent > 0.95 else "normal",
                "gpu_temperature_status": "high" if state.gpu_temperature > 85 else "normal",
                "cpu_utilization_status": "high" if state.cpu_percent > 95 else "normal"
            },
            "alerts": alerts,
            "recommendations": recommendations
        }
    
    def _analyze_training_stability(self, state: TrainingState) -> Dict[str, Any]:
        """Analyze training stability and convergence"""
        if len(self.history) < 20:
            return {"stability_analysis": "insufficient_data"}
        
        recommendations = []
        alerts = []
        
        # Gradient norm analysis
        recent_grads = [h.get("gradient_norm", 0) for h in list(self.history)[-10:]]
        
        if recent_grads and max(recent_grads) > 100:
            alerts.append(AgentAlert(
                alert_type="gradient_explosion",
                severity="critical",
                message=f"Gradient explosion detected: max norm {max(recent_grads):.2f}",
                timestamp=datetime.now(),
                source_agent=self.name,
                recommended_actions=[
                    AgentAction(
                        action_type="emergency_gradient_clip",
                        description="Apply emergency gradient clipping",
                        priority="high",
                        auto_executable=True,
                        parameters={"max_norm": 1.0}
                    ),
                    AgentAction(
                        action_type="reduce_learning_rate_emergency",
                        description="Emergency learning rate reduction",
                        priority="high",
                        auto_executable=True,
                        parameters={"factor": 0.1}
                    )
                ]
            ))
        
        # Learning rate analysis
        if state.learning_rate < 1e-8:
            alerts.append(AgentAlert(
                alert_type="learning_rate_too_low",
                severity="warning",
                message=f"Learning rate very low: {state.learning_rate:.2e}",
                timestamp=datetime.now(),
                source_agent=self.name,
                recommended_actions=[
                    AgentAction(
                        action_type="learning_rate_warmup",
                        description="Apply learning rate warmup",
                        priority="medium",
                        auto_executable=False,
                        parameters={"warmup_steps": 100}
                    )
                ]
            ))
        
        return {
            "stability_analysis": {
                "gradient_stability": "unstable" if max(recent_grads) > 100 else "stable",
                "learning_rate_status": "too_low" if state.learning_rate < 1e-8 else "normal"
            },
            "alerts": alerts,
            "recommendations": recommendations
        }
    
    def _analyze_convergence_patterns(self, state: TrainingState) -> Dict[str, Any]:
        """Analyze convergence patterns and predict training outcomes"""
        if len(self.history) < 50:
            return {"convergence_analysis": "insufficient_data"}
        
        recommendations = []
        recent_losses = [h.get("loss", 0) for h in list(self.history)[-50:]]
        
        # Simple convergence detection
        if len(recent_losses) >= 20:
            recent_trend = np.polyfit(range(len(recent_losses[-20:])), recent_losses[-20:], 1)[0]
            
            convergence_status = "converging" if recent_trend < -0.001 else "plateaued"
            
            if convergence_status == "plateaued":
                recommendations.append(
                    "Training appears to have plateaued. Consider adjusting learning rate or model architecture."
                )
        
        return {
            "convergence_analysis": {
                "status": convergence_status,
                "trend_slope": recent_trend,
                "recommendations": recommendations
            }
        }

class WeaveAgenticFramework:
    """Comprehensive agentic framework for training monitoring"""
    
    def __init__(self, project_name: str = "codelupe-agentic"):
        self.project_name = project_name
        self.agents = {}
        self.global_state = TrainingState()
        self.action_queue = deque()
        self.alert_history = deque(maxlen=500)
        self.auto_actions_enabled = True
        
        if WEAVE_AVAILABLE:
            weave.init(f"{project_name}-agents")
            self.weave_enabled = True
        else:
            self.weave_enabled = False
            print("âš ï¸  Weave not available - agentic framework running in limited mode")
        
        # Initialize core agents
        self._initialize_core_agents()
        
        print(f"ðŸ¤– Weave Agentic Framework Initialized")
        print(f"   Project: {project_name}")
        print(f"   Agents: {len(self.agents)}")
        print(f"   Weave: {'âœ…' if self.weave_enabled else 'âŒ'}")
    
    def _initialize_core_agents(self):
        """Initialize core monitoring agents"""
        
        # Loss optimization agent
        self.agents["loss_optimizer"] = TrainingIntelligenceAgent("loss_optimizer", "optimization")
        
        # Resource management agent
        self.agents["resource_manager"] = TrainingIntelligenceAgent("resource_manager", "resource")
        
        # Stability monitoring agent
        self.agents["stability_monitor"] = TrainingIntelligenceAgent("stability_monitor", "stability")
        
        # Performance optimization agent
        self.agents["performance_optimizer"] = TrainingIntelligenceAgent("performance_optimizer", "performance")
        
        # Safety monitoring agent
        self.agents["safety_monitor"] = TrainingIntelligenceAgent("safety_monitor", "safety")
        
        print(f"âœ… {len(self.agents)} core agents initialized")
    
    @weave.op() if WEAVE_AVAILABLE else lambda f: f
    def process_training_step(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Process training step through all agents"""
        
        # Update global state
        self.global_state = TrainingState(
            current_step=metrics.get("step", 0),
            current_epoch=metrics.get("epoch", 0),
            loss=metrics.get("loss", 0.0),
            learning_rate=metrics.get("learning_rate", 0.0),
            gradient_norm=metrics.get("gradient_norm", 0.0),
            gpu_memory_percent=metrics.get("gpu_memory_percent", 0.0),
            gpu_temperature=metrics.get("gpu_temperature", 0.0),
            cpu_percent=metrics.get("cpu_percent", 0.0),
            training_speed=metrics.get("training_speed", 0.0)
        )
        
        # Process through all agents
        agent_responses = {}
        all_alerts = []
        all_recommendations = []
        all_actions = []
        
        for agent_name, agent in self.agents.items():
            try:
                response = agent.analyze_training_state(self.global_state)
                agent_responses[agent_name] = response
                
                # Collect alerts
                if "alerts" in response:
                    all_alerts.extend(response["alerts"])
                
                # Collect recommendations
                if "recommendations" in response:
                    all_recommendations.extend(response["recommendations"])
                
                # Collect actions
                if "actions" in response:
                    all_actions.extend(response["actions"])
                    
            except Exception as e:
                print(f"âš ï¸  Agent {agent_name} error: {e}")
                agent_responses[agent_name] = {"error": str(e)}
        
        # Process alerts and actions
        critical_alerts = [a for a in all_alerts if hasattr(a, 'severity') and a.severity == "critical"]
        auto_actions = [a for a in all_actions if hasattr(a, 'auto_executable') and a.auto_executable]
        
        # Execute auto actions if enabled
        executed_actions = []
        if self.auto_actions_enabled and auto_actions:
            for action in auto_actions[:3]:  # Limit to 3 auto actions per step
                try:
                    result = self._execute_action(action)
                    executed_actions.append({"action": action, "result": result})
                except Exception as e:
                    print(f"âš ï¸  Auto action failed: {e}")
        
        # Store alerts in history
        self.alert_history.extend(all_alerts)
        
        # Prepare response
        framework_response = {
            "timestamp": datetime.now().isoformat(),
            "global_state": asdict(self.global_state),
            "agent_responses": agent_responses,
            "summary": {
                "total_alerts": len(all_alerts),
                "critical_alerts": len(critical_alerts),
                "recommendations": len(all_recommendations),
                "auto_actions_executed": len(executed_actions)
            },
            "executed_actions": executed_actions,
            "framework_health": "active"
        }
        
        return framework_response
    
    def _execute_action(self, action: AgentAction) -> Dict[str, Any]:
        """Execute an agent action"""
        result = {
            "action_type": action.action_type,
            "executed": False,
            "timestamp": datetime.now().isoformat(),
            "result": None
        }
        
        try:
            if action.action_type == "reduce_batch_size":
                # This would integrate with your training loop
                result["result"] = "Batch size reduction signal sent"
                result["executed"] = True
                
            elif action.action_type == "emergency_gradient_clip":
                # This would integrate with your optimizer
                result["result"] = "Gradient clipping enabled"
                result["executed"] = True
                
            elif action.action_type == "reduce_learning_rate":
                # This would integrate with your scheduler
                result["result"] = "Learning rate reduction signal sent"
                result["executed"] = True
                
            else:
                result["result"] = f"Action type {action.action_type} not implemented"
                
        except Exception as e:
            result["error"] = str(e)
        
        return result
    
    def get_agent_insights(self) -> Dict[str, Any]:
        """Get comprehensive insights from all agents"""
        insights = {
            "timestamp": datetime.now().isoformat(),
            "active_agents": len(self.agents),
            "total_alerts_history": len(self.alert_history),
            "recent_alerts": list(self.alert_history)[-10:],
            "agent_status": {}
        }
        
        for agent_name, agent in self.agents.items():
            insights["agent_status"][agent_name] = {
                "alerts_generated": len(agent.alerts_generated),
                "actions_taken": len(agent.actions_taken),
                "history_size": len(agent.history),
                "status": "active"
            }
        
        return insights
    
    def enable_auto_actions(self, enabled: bool = True):
        """Enable or disable automatic action execution"""
        self.auto_actions_enabled = enabled
        print(f"ðŸ¤– Auto actions {'enabled' if enabled else 'disabled'}")
    
    def add_custom_agent(self, agent: TrainingIntelligenceAgent):
        """Add a custom agent to the framework"""
        self.agents[agent.name] = agent
        print(f"âœ… Custom agent '{agent.name}' added")

# Integration with comprehensive monitor
class AgenticMonitorIntegration:
    """Integration layer between comprehensive monitor and agentic framework"""
    
    def __init__(self, monitor, framework: WeaveAgenticFramework):
        self.monitor = monitor
        self.framework = framework
        self.integration_active = True
    
    def process_metrics(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Process metrics through agentic framework"""
        if not self.integration_active:
            return {}
        
        try:
            # Process through agents
            agent_response = self.framework.process_training_step(metrics)
            
            # Log agent insights to W&B
            if self.monitor.wandb_run:
                agent_summary = {
                    "agent_alerts": agent_response["summary"]["total_alerts"],
                    "agent_critical_alerts": agent_response["summary"]["critical_alerts"],
                    "agent_recommendations": agent_response["summary"]["recommendations"],
                    "agent_auto_actions": agent_response["summary"]["auto_actions_executed"],
                    "framework_health": agent_response["framework_health"]
                }
                self.monitor.wandb_run.log(agent_summary, step=metrics.get("global_step", 0))
            
            return agent_response
            
        except Exception as e:
            print(f"âš ï¸  Agentic integration error: {e}")
            return {"error": str(e)}

# Factory function
def create_agentic_framework(project_name: str = "codelupe-agentic") -> WeaveAgenticFramework:
    """Create and initialize agentic framework"""
    return WeaveAgenticFramework(project_name)

if __name__ == "__main__":
    # Test the agentic framework
    print("ðŸ§ª Testing Weave Agentic Framework")
    
    # Create framework
    framework = create_agentic_framework("test-agentic")
    
    # Simulate training steps
    for step in range(10):
        test_metrics = {
            "step": step,
            "epoch": 0,
            "loss": max(0.1, 2.0 - step * 0.2),  # Decreasing loss
            "learning_rate": 2e-5,
            "gradient_norm": 0.5 + step * 0.1,
            "gpu_memory_percent": min(0.98, 0.3 + step * 0.07),  # Increasing memory
            "gpu_temperature": 70 + step * 2,
            "cpu_percent": 50 + step * 5
        }
        
        print(f"\\nStep {step}:")
        response = framework.process_training_step(test_metrics)
        print(f"  Alerts: {response['summary']['total_alerts']}")
        print(f"  Auto Actions: {response['summary']['auto_actions_executed']}")
        
        time.sleep(0.1)  # Simulate processing time
    
    # Get insights
    insights = framework.get_agent_insights()
    print(f"\\nðŸ“Š Framework Insights:")
    print(f"  Active Agents: {insights['active_agents']}")
    print(f"  Total Alerts: {insights['total_alerts_history']}")
    
    print("âœ… Agentic framework test completed")
